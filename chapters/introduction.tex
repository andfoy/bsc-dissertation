%!TEX root = ../dissertation.tex
\chapter{Introduction}
\label{introduction}
Since 2012 \cite{krizhevsky2012imagenet}, the advent of deep learning has enabled researchers to study classical problems in computer vision such as object detection, semantic segmentation and instance segmentation, and achieve high performances in challenging datasets. These problems have lately been tackled using Convolutional Neural Networks (CNNs) \cite{DBLP:journals/corr/LongSD14}\cite{zhao2017pspnet}\cite{he_mask_2017} and insights into the information flow within such networks \cite{DBLP:journals/corr/HeZRS15}\cite{DBLP:journals/corr/HuangLW16a}, bearing in mind that CNNs are particularly well suited to process spatial and hierarchical information. 

On the other hand, Recurrent Neural Network (RNNs) models have been used previously to model and predict time sequences, these type of models have presented an state-of-the-art performance on different temporal-dependent applications, such as Speech Recognition \cite{DBLP:journals/corr/abs-1303-5778}, Handwriting recognition \cite{DBLP:journals/corr/Graves13}, text analysis and other specialized applications, such as protein reconstruction and nucleotide prediction \cite{doi:10.1093/bioinformatics/btw678}, and more recently on more advanced tasks such as Answering Open Domain Questions from large scale text corpus such as Wikipedia \cite{chen2017reading}. Moreover, several RNN models have been proposed, such as Long Short Term Memory (LSTM) \cite{Hochreiter:1997:LSM:1246443.1246450} units, Gated Recurrent Units (GRU) \cite{DBLP:journals/corr/ChoMGBSB14} and Simple Recurrent Units (SRU) \cite{DBLP:journals/corr/abs-1709-02755}, which presents the best time performance with respect to the previous models. However, all the aforementioned models are based on a set of state equations that relate both present input information at time $t$ with information from previous time steps
