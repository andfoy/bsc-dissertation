%!TEX root = ../dissertation.tex

\chapter{Experiments and Results}

\newthought{To evaluate the performance of the DMN}, several experiments were proposed to fine-tune and select the best values for each component of the network. All the experimental set-up and guidelines, as well the datasets used are based on those used by previous works \cite{hu2016segmentation, liu2017segmentation}.   There are currently four datasets available to model this task: ReferIt, UNC, UNC+ \cite{KazemzadehOrdonezMattenBergEMNLP14} and GRef \cite{DBLP:journals/corr/MaoHTCYM15}. Each dataset is different from each other in terms of the type of objects referred, the complexity of the referral expression and also their length. While the first dataset is based on the IAPR-12 \cite{ESCALANTE2010419} image segmentation collection, the last three ones are based on MS COCO \cite{DBLP:journals/corr/LinMBHPRDZ14}, and therefore they expose the same variety of objects. Each dataset complete description is presented on section \ref{section:datasets}

\section{Datasets}
\label{section:datasets}
\subsection{ReferIt}
\textbf{ReferIt} \cite{KazemzadehOrdonezMattenBergEMNLP14} is a crowd-sourced database that contains images and referring expressions to objects in those images. Currently it has 130,525 expressions, referring to 96,654 distinct objects, in 19,894 photographs of natural scenes.

\subsection{GRef}
\textbf{GRef}, or RefCOCOg \cite{DBLP:journals/corr/MaoHTCYM15}, was collected on Amazon's Mechanical Turk and contains 85,474 referring expressions for 54,822 objects in 26,711 images selected to contain between two and four objects of the same object class \cite{DBLP:journals/corr/YuPYBB16}, which means that an expression making reference to a determined type of object will need to be further read to determine \textit{which} object the query is referring to, since ambiguity arises when only guided by semantic class cues.

\subsection{UNC}
\textbf{UNC}, or RefCOCO, was collected interactively in the ReferIt game, with images that were selected to contain two or more objects of the same object category, so that it presents similar challenges to those of RefCOCOg. It consists of 142,209 refer expressions for 50,000 objects in 19,994 images \cite{DBLP:journals/corr/YuPYBB16}.

\subsection{UNC+}
\textbf{UNC+}, or RefCOCO+, is similar to UNC but has an additional restriction regarding words describing location: expressions must be based only on appearance rather than location \cite{DBLP:journals/corr/YuPYBB16}. Such restriction implies that the expression will depend on the perspective of the scene and the semantic class of the object.


\section{Metrics}
Experiments with the proposed method were performed on the four standard datasets described above by training on the training set and evaluating the performance in each of the validation or test sets. Evaluation is performed by using two standard metrics: \textit{(i)} mean Intersection over Union (mIoU), defined as the total intersection area between the output and Ground Truth (GT) mask, divided by the total union between the output and GT mask, added over all the images in the evaluation set, and \textit{(ii)} \textit{Precision}$@X$, or \textit{Pr}$@X$, ($X \in \{0.5, 0.6, 0.7, 0.8, 0.9\}$), defined as the percentage of images with IoU higher than $X$. The usage of these metrics allows for direct comparison with previous works.

\section{Architecture Experiments}
To determine the optimal values for each hyperparameter defined on the DMN, a set of experiments was done, each one of them varies each parameter independently to establish the importance and weight on the model (Subection~\ref{section:base}). Besides, on Subsection \ref{section:refinement} follow-up experiments were done on different combinations of the parameters of the base model. Due to the variety in the number of available datasets, all the base and refinement experiments are based only on UNC, as it contains more training cases and more complex queries than ReferIt, which may contain phrases that refer to a single object on a single word.

All the experiments presented on the current and follow-up sections are trained using a learning rate of \num{1e-5} and Adam \cite{DBLP:journals/corr/KingmaB14} optimiser with batch size 1 and a total number of 24 epochs during the low resulution phase and a further 10 epochs on high resolution. The model reference implementation was done on PyTorch\footnote{\url{http://pytorch.org}} and it is available at: \url{https://github.com/andfoy/query-objseg}

%Then, the resulting model architecture (according to the previous experiments results) is fine-tuned on each of the remaining datasets, using the pretrained weights on UNC

\subsection{Base Experiments}
\label{section:base}
On Table~\ref{Tab:Arch_Exps}, the main values for each of the base experiments are presented, only the parameters referred to the main aspects of the Language and the Syntesis modules of the model are subject to experimentation. For instance, these experiments only consider variations on the number of layers of each of the recurrent modules involved on the model, as well on their hidden size representations. The experiments also consider variations on the total number of dynamical filters trained on the SM. Finally, they also consider the inclusion of the WE representation at the moment of computing both the language filters and the multimodal representation.

These experiments do not consider variations on the Visual Module, as its functionality is provided by a state-of-the-art CNN, which should provide the best visual representation as they were tested previously on large-scale visual competitions and datasets, such as ImageNet. Also, they do not consider modifications on the Upsampling module, as the model is trained on two phases, one to learn a low resolution segmentation map, and another lo learn the final upsampling output. However, variations on this module are provided as part of the Ablation Experiments described on section \ref{section:ablation}.

\begin{table}[!htbp]
    \centering
    \begin{tabular}{|c|c|}
         &  \\
         & 
    \end{tabular}
    \caption{Caption}
    \label{Tab:Arch_Exps}
\end{table}

\subsection{Refinement Experiments}