%!TEX root = ../dissertation.tex

\chapter{Experiments and Results}

\newthought{To evaluate the performance of the DMN}, several experiments were proposed to fine-tune and select the best values for each component of the network. All the experimental set-up and guidelines, as well the datasets used are based on those used by previous works \cite{hu2016segmentation, liu2017segmentation}.   There are currently four datasets available to model this task: ReferIt, UNC, UNC+ \cite{KazemzadehOrdonezMattenBergEMNLP14} and GRef \cite{DBLP:journals/corr/MaoHTCYM15}. Each dataset is different from each other in terms of the type of objects referred, the complexity of the referral expression and also their length. While the first dataset is based on the IAPR-12 \cite{ESCALANTE2010419} image segmentation collection, the last three ones are based on MS COCO \cite{DBLP:journals/corr/LinMBHPRDZ14}, and therefore they expose the same variety of objects. Each dataset complete description is presented on section \ref{section:datasets}

\section{Datasets}
\label{section:datasets}
\subsection{ReferIt}
\textbf{ReferIt} \cite{KazemzadehOrdonezMattenBergEMNLP14} is a crowd-sourced database that contains images and referring expressions to objects in those images. Currently it has 130,525 expressions, referring to 96,654 distinct objects, in 19,894 photographs of natural scenes.

\subsection{GRef}
\textbf{GRef}, or RefCOCOg \cite{DBLP:journals/corr/MaoHTCYM15}, was collected on Amazon's Mechanical Turk and contains 85,474 referring expressions for 54,822 objects in 26,711 images selected to contain between two and four objects of the same object class \cite{DBLP:journals/corr/YuPYBB16}, which means that an expression making reference to a determined type of object will need to be further read to determine \textit{which} object the query is referring to, since ambiguity arises when only guided by semantic class cues.

\subsection{UNC}
\textbf{UNC}, or RefCOCO, was collected interactively in the ReferIt game, with images that were selected to contain two or more objects of the same object category, so that it presents similar challenges to those of RefCOCOg. It consists of 142,209 refer expressions for 50,000 objects in 19,994 images \cite{DBLP:journals/corr/YuPYBB16}.

\subsection{UNC+}
\textbf{UNC+}, or RefCOCO+, is similar to UNC but has an additional restriction regarding words describing location: expressions must be based only on appearance rather than location \cite{DBLP:journals/corr/YuPYBB16}. Such restriction implies that the expression will depend on the perspective of the scene and the semantic class of the object.


\section{Metrics}
Experiments with the proposed method were performed on the four standard datasets described above by training on the training set and evaluating the performance in each of the validation or test sets. Evaluation is performed by using two standard metrics: \textit{(i)} mean Intersection over Union (mIoU), defined as the total intersection area between the output and Ground Truth (GT) mask, divided by the total union between the output and GT mask, added over all the images in the evaluation set, and \textit{(ii)} \textit{Precision}$@X$, or \textit{Pr}$@X$, ($X \in \{0.5, 0.6, 0.7, 0.8, 0.9\}$), defined as the percentage of images with IoU higher than $X$. The usage of these metrics allows for direct comparison with previous works.

\section{}